<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MagicDance: Realistic Human Dance Video Generation
with Motions & Facial Expressions Transfer.">
  <meta name="keywords" content="Controllable Human Dance, Video Generation, Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Effective Adapter for Face Recognition in the Wild</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Effective Adapter for Face Recognition in the Wild</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://liuyunhaozz.github.io/">Yunhao Liu</a><sup>1</sup>,&nbsp;&nbsp;</span> 
            <span class="author-block">
              <a href="http://luqi.info/">Lu Qi</a><sup>2</sup>,&nbsp;&nbsp;</span>
            <span class="author-block">
              <a href="https://liagm.github.io/">Yu-Ju Tsai</a><sup>2</sup>,&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://lxtgh.github.io/">Xiangtai Li</a><sup>3</sup>,&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://ckkelvinchan.github.io/">Kelvin C.K. Chan</a><sup>4</sup>, &nbsp;&nbsp;
            </span><br>
            <span class="author-block">
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><sup>2,4</sup>,&nbsp;&nbsp;
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Dalian University of Technology, </span>
            <span class="author-block"><sup>2</sup>The University of California, Merced, </span>
            <span class="author-block"><sup>3</sup>Nanyang Technology University, </span>
            <span class="author-block"><sup>4</sup>Google Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.01734.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.01734"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=IBAY7mSVpfw&ab_channel=yunhaoLiu"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/liuyunhaozz/faceadapter/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/faceadapter.jpeg" alt="empty">
        <p>
          <br />
          In real-world applications, face recognition systems frequently encounter probe images of low quality (LQ), which presents a significant domain gap compared to the high-quality (HQ) embedding gallery. Our method (c) addresses this challenge by integrating the features from the LQ images with those of enhanced HQ images in the fusion structure. Compared with conventional methods (a) (b), our method effectively bridges the domain gap, ensuring more accurate and reliable face recognition performance in real-world conditions.
        </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we tackle the challenge of face recognition in the wild, where images often suffer from low quality and real-world distortions. Traditional heuristic approaches—either training models directly on these degraded images or their enhanced counterparts using face restoration techniques—have proven ineffective, primarily due to the degradation of facial features and the discrepancy in image domains. To overcome these issues, we propose an effective adapter for augmenting existing face recognition models trained on high-quality facial datasets. The key of our adapter is to process both the unrefined and the enhanced images by two similar structures where one is fixed and the other trainable. Such design can confer two benefits. First, the dual-input system minimizes the domain gap while providing varied perspectives for the face recognition model, where the enhanced image can be regarded as a complex non-linear transformation of the original one by the restoration model. Second, both two similar structures can be initialized by the pre-trained models without dropping the past knowledge. The extensive experiments in zero-shot settings show the effectiveness of our method by surpassing baselines of about 3%, 4%, and 7% in three datasets. Our code will be publicly available at <a href="https://github.com/liuyunhaozz/FaceAdapter/" target="_blank">https://github.com/liuyunhaozz/FaceAdapter/</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="./static/images/demo/final-pre-video.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" sandbox="" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>

        <!-- Interpolating. -->
        <img src="./static/images/method-network.jpeg" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          <br />
          <b>Joint Face Recognition Framework with Dual-Input Processing.</b> This architecture processes both low-quality (LQ) and restored high-quality (HQ) images, extracting them by two identical face recognition models. The Fusion Structure integrates feature sets before passing them to an Angular Margin Softmax function for loss computation, optimizing the network for enhanced recognition accuracy.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>
    <!--/ Method. -->
    <br />
    <!-- Results -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
        <h3 class="title is-4">1. Comparison to <a href="https://insightface.ai/arcface" target="_blank">Arcface</a> on image pair</h3>
        <img src="./static/images/compare-A.jpeg" class="interpolation-image" alt="Empty"/>
        <br />
        <p>
          Comparative analysis of face recognition methods on image pair. (a) illustrates the comparison results using different methods on an image pair of the same person, while (b) presents the results for an image pair of different people. Each image pair is accompanied by a cosine similarity score, ranging from -1 to 1, where a higher score indicates greater similarity between the images, and vice versa.
        </p>
        <br />
 
        <!-- <br />
        <br />
        <br />
        <br />
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/demo/pose.gif"
                 alt="Interpolate start reference image."
                 width="300" />
            <p>Condition</p>
          </div>
          <div class="column has-text-centered">
            <img src="./static/images/demo/ref_1.jpeg"
                 alt="Interpolation end reference image."
                 width="100%" />
            <p>Reference</p>
          </div>
        </div>
        <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
          <source src="./static/images/demo/gen_1.mp4" type="video/mp4">
        </video>
        <p><center>Generated video</center></p>
        <br /> -->
        <br />
        <h3 class="title is-4">2. Comparison to <a href="https://insightface.ai/arcface" target="_blank">Arcface</a> on 1:N matching </h3>
        <img src="./static/images/compare-B.jpeg" class="interpolation-image" alt="Empty"/>
        <br />
        <p>
          <br />
          Comparative analysis of face recognition methods on 1:N matching. This figure displays the Top 5 matching results for blurred face probes using both Arcface and our method. Each line contains an input probe and the Top 5 matches of the gallery using each of the two methods. 
        </p>
        <!-- <br /> -->

        <!-- <br />
        <br />
        <br />
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/demo/pose.gif"
                 alt="Interpolate start reference image."
                 width="130" />
            <p>Condition</p>
          </div>
          <div class="column has-text-centered">
            <img src="./static/images/demo/civitai.png"
                 alt="Interpolation end reference image."
                 width="1000" />
            <p>Reference</p>
          </div>
        </div>
        <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
          <source src="./static/images/demo/civitai.mp4" type="video/mp4">
        </video>
        <p><center>Generated video</center></p>
        <br /> -->

    <!--/ Results. -->
    <!-- <br /> -->
    <!-- Compare -->
    <!--/ Compare. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> @misc{liu2023effective,
        title={Effective Adapter for Face Recognition in the Wild}, 
        author={Yunhao Liu and Lu Qi and Yu-Ju Tsai and Xiangtai Li and Kelvin C. K. Chan and Ming-Hsuan Yang},
        year={2023},
        eprint={2312.01734},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
  } </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
